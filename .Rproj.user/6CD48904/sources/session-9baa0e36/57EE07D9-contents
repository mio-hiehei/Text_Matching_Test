#' ---
#' title: "Politische Vierteljahresschrift - Gender Matching"
#' author: "Katelyn Nutley, Ayjeren Bekmuratovna R., Ingo Rohlfing"
#' date: "`r sys.Date()`"
#' output: 
#'    html_document:
#'       toc: true
#'       toc_depth: 3
#'       toc_float: true
#'       code_folding: show
#' ---

#' ### Part I: Setup ### 

#' ## Step 1: Knitr ##
#+ knitr-setup, include = F----
knitr::opts_chunk$set(warning = TRUE,
                      message = FALSE)

#' ## Step 2: Load Libraries
#' Library are in alphabetical order.
#+ packages----
library(tidyverse)
library(stm)
library(cem)


#' ## Step 3: Load the data and text matching functions
#+ datasets, eval = F ----
load("./PVS/data_export/PVS_FINAL.RData")
source(file = "./PVS/code/rn_functions.R")

#+ adding auth type, eval = F ----
# this script aims at adding a unique id that allows
# to tag an article as authored my men/man only vs. women/woman only
# adding a new column that adds an id to articles

pvs_final <- pvs_final %>%
  group_by(doi) %>%
  mutate(group_id = cur_group_id())

# counting the gender per article
pvs_group_gender <- pvs_final %>% 
  group_by(group_id) %>% 
  count(pred_gender)

colnames(pvs_group_gender)[3] <- "authors_count"

# replacing the letter values with numbers
pvs_group_gender$pred_gender <- as.factor(pvs_group_gender$pred_gender)

pvs_group_gender <- pvs_group_gender %>% 
  mutate(pg = case_when(pred_gender == "M" ~ 1,
                        pred_gender == "F" ~ 2,
                        pred_gender == "?F" ~ 2,
                        pred_gender == NA ~ 100))
pvs_group_gender$pg <- as.numeric(pvs_group_gender$pg)

# Now, mixed gender groups will have a value of 3!
pvs_auth_type <- pvs_group_gender %>% 
  group_by(group_id) %>% 
  summarise(au_type = sum(pg))

# Translating the numeric values back into character values
pvs_auth_type <- pvs_auth_type %>% 
  mutate(au_type = case_when(au_type == 1 ~ "male_only",
                             au_type == 2 ~ "female_only",
                             au_type == 3 ~ "mixed"))

pvs_auth_type$au_type <- as.factor(pvs_auth_type$au_type)


# adding the auth_type to the pvs_grouped
pvs_combined <- left_join(pvs_group_gender, pvs_auth_type)

# adding the auth_type to the final data
pvs_select <- pvs_combined %>% 
  dplyr::select(group_id, au_type)

# The final dataset with the au_type variable
# which codes whether the article is authored by men only, 
# women only, or mixed gender

pvs_final <- left_join(pvs_final, pvs_select)


#' ## Step 4: Topic modeling
#+ preprocess for stm, eval = F ----

pvs_final$treat <- ifelse(pvs_final$au_type=="female_only", 1, 0)
pvs_final <- pvs_final %>% 
  drop_na()

# Processing the data for the modeling

#processed <- textProcessor(pvs_final$Text,
#                            metadata = pvs_final,  
#                            removestopwords = TRUE,  
#                            removenumbers = TRUE, 
#                            removepunctuation = TRUE, 
#                            verbose = TRUE, 
#                            language = "english") 
# 
# # The stm object generation
# out <- prepDocuments(processed$documents, 
#                      processed$vocab, 
#                      processed$meta)  
# 
# 
# # Saving the output object details
# docs <- out$documents 
# vocab <- out$vocab
# meta <- out$meta

# Processing with subset
processed <- textProcessor(pvs_final$Text, 
                           metadata=pvs_final,
                           removestopwords = TRUE,  
                           removenumbers = TRUE, 
                           removepunctuation = TRUE, 
                           verbose = TRUE, 
                           language = "german")
#
processed$meta$text <- pvs_final$Text

## run prepDocuments to filter this down to words which appear
## in at least 26 articles
out <- prepDocuments(processed$documents, processed$vocab, 
                     processed$meta, lower.thresh=270)
rm(processed)

#' Please, beware that the following script can last for an hour
#' or longer depending on the computer

K <- 15 #the number of topics we use throughout

## 33 Mins

  t0 <- Sys.time()
  stm.out.c <- stm(out$documents, out$vocab, K=K, prevalence=~
                     treat + volume + issue, content=~treat,data=out$meta,
                   max.em.its=100, 
                   seed=1033311)
  t1 <- Sys.time()
  save(stm.out.c, file="./PVS/data_export/stm_fit.RData")
  t1-t0 # Time difference of 12 mins


summary(stm.out.c)




## Re-estimate under treatment

  t0 <- Sys.time()
  betaindex <- rep(2,length(stm.out.c$settings$covariates$betaindex))
  beta <- list(beta=lapply(stm.out.c$beta$logbeta, exp))
  if(!is.null(stm.out.c$beta$kappa)) beta$kappa <- stm.out.c$beta$kappa
  sigma <- stm.out.c$sigma
  lambda <- stm.out.c$eta
  suffstats <- stm:::estep(documents=out$documents,
                           beta.index=betaindex,
                           update.mu=(!is.null(stm.out.c$mu$gamma)),
                           beta$beta,
                           lambda, stm.out.c$mu$mu,
                           sigma,
                           verbose=T)
  lambda <- cbind(suffstats$lambda,0)
  theta <- exp(lambda - stm:::row.lse(lambda))
  rm(betaindex,beta,sigma,lambda, suffstats)
  save(theta, file="./PVS/data_export/STM_15TopicsContentThetas.RData")
  t1 <- Sys.time()
  t1-t0 # 9 mins


  ##Generate the projections

    t0 <- Sys.time()
    stmproj.interact.theta <- project(stm.out.c, out$documents,
                                      interaction=F)#, int.type="theta")
    t1 <- Sys.time()
    save(stmproj.interact.theta, file="./PVS/data_export/STM_15TopicsContent_ThetaProjection.RData")
    t1-t0 # Time difference of 2.52184 secs



#########
#Run DMR#
#########
# quick function to convert our tdm to MNIR's
doc.to.tdm <- function(documents, vocab){
  tdm <- matrix(0,nrow=length(documents),
                ncol=length(vocab))
  for(i in 1:length(documents)){
    tdm[i,documents[[i]][1,]] <-
      documents[[i]][2,]
  }
  return(tdm)
}

tdm <- doc.to.tdm(out$documents, out$vocab)
    

## MNIR fit

  ## Multinomial inverse regression fit.  
  ## Treatment is out$meta$all_female.
  fit <- textir::mnlm(NULL,
                      covars=cbind(out$meta$treat),
                      counts=tdm)
  save(fit, file="./PVS/data_export/Treat_DMROut2.RData")


  ##############################
  #Add STM topics and DMR topics to the metadata
  ##############################
  #Finding the projection for the data
  proj <- textir::srproj(fit, tdm)
  out$meta$dmr1 <- proj[,1]
  out$meta$dmr2 <- proj[,2]
  out$meta$proj.interact1 <- stmproj.interact.theta$projection[[1]]
  out$meta$proj.interact2 <- stmproj.interact.theta$projection[[2]]
  #Put the topics into the metadata
  out$meta[,paste("Topic", 1:K, sep="")] <- theta
  
  

  
##############################
#Match just on topics
##############################
  
breaks <- list()
for(i in 1:K){
  breaks[[paste("Topic", i, sep="")]] <- c(0,.1,1)
}
breaks$article_age <- 5

match.out <- cem::cem("treat", out$meta,
                       drop=names(out$meta)[!names(out$meta)%in%c("treat", "issue", "volume", paste("Topic", 1:K, sep=""))], 
                      cutpoints=breaks,  method="binary")
  
m.data <- out$meta[match.out$matched==T,]
m.data$strata <- match.out$strata[match.out$matched==T]
m.data$weights <- match.out$w[match.out$matched==T]
match.tdm.t <- tdm[out$meta$id%in%m.data$id,]
  
  




#+ fitting the stm, eval = F ----

TopicsFit <- stm(out$documents, # specify the documents
                 out$vocab, # specify the vocabulary
                 K=5, # specify the number of topics we expect (I chose this
                 # randomly. Number of topics is one of the most criticized part
                 # of structural topic modeling (!)
                 prevalence = ~volume + issue,
                 content = ~au_type, 
                 max.em.its=30, 
                 data=out$meta,
                 seed=8458159)  

# Playing with K a little
K = 10
TopicsFit_10 <- stm(out$documents, out$vocab, K=K, 
                    prevalence=~treat,
                    content = ~treat,
                    max.em.its=75, data=out$meta, 
                    seed=8458159)
summary(TopicsFit_10)

K = 15
TopicsFit_15 <- stm(out$documents, out$vocab, K=K, 
                    prevalence=~treat,
                    content = ~treat,
                    max.em.its=75, data=out$meta, 
                    seed=8458159)



#' ## Step 5: Text matching
#+ text matching, eval = F ----
# Refitting the model under the condition
refitted <- refit(stm.out.c, 
                  out$docs, # the docs (document feature matrix)
                  content_level="1") # specifying that we want to rerun the model

projection <- project(TopicsFit, docs, interactions = FALSE)

# projection_interacted <- project(TopicsFit, docs, interactions = TRUE)

# matching the data
matched <- cem_match(refitted, projection=projection, meta$treat,
                     projection_breaks=2)


# Adding a boolean to identify if an observation is in the matched
# data or not (only for 5 topics)
rn_pilot_sample$matched <- matched$matched


# sub-setting for matched data
match_only <- rn_pilot_sample %>% 
  select(-text, -cleaned_text) %>% 
  filter(matched == TRUE)

# Turning relevant cem matching elements into a dtf
match_params <- matched[c("w", "strata", "matched")] %>% 
  as.data.frame() %>% 
  filter(matched == TRUE)

# Adding matching-related metadata to the final output
match_only <- cbind(match_only, match_params)
match_only$matched <- NULL

# Histogram for the weights
ggplot(match_only, aes(w))+
  geom_bar()+ 
  scale_x_binned()




